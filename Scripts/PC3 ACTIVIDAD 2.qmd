---
title: "SEMANA 12 G1"
format: html
editor: visual
---

Integrantes

Dayana Lilian Marroquín Ticona

Alexandra Pamela Degregori Hinojosa

Cayllahua Huarancca Mayrha

Leidy Yojhana Loayza Quispe.

Evelyn Lizbeth Quispe Valencia

### Instalar (si es necesario)

```{r}
install.packages("performance")
```

### Cargar paquetes

```{r}
library(tidyverse)
library(here)
library(rio)
library(gtsummary)
library(car)
library(survival)
library(performance)
```

## 1 Modelos univariados (no ajustados) vs. multivariados (ajustados)

Hasta ahora, hemos explorado modelos de regresión que evalúan un predictor a la vez. A estos se les denomina modelos univariados o no ajustados, ya que solo consideran una variable predictora. Sin embargo, datasets utilizados en estas sesiones, al igual que muchos datos que probablemente recolectes, provienen de estudios observacionales. Es decir, no existe un control estricto sobre qué individuos se incluyen en el análisis y cuáles no. Esto implica que múltiples factores pueden influir en el desenlace de interés de manera simultánea.

Por esta razón, no es adecuado extraer conclusiones definitivas a partir de modelos no ajustados, ya que estos ignoran el efecto de posibles variables de confusión. En su lugar, es necesario realizar un análisis multivariado o ajustado, que permita considerar de manera simultánea varios predictores potenciales.

Por ejemplo, es poco probable que solo el tipo de accidente cerebrovascular (ACV) —isquémico o hemorrágico— determine la probabilidad de fallecer tras un evento de este tipo. Factores como la edad, el sexo, las comorbilidades preexistentes y los hábitos de vida también pueden afectar de manera importante este riesgo. Ignorar estas variables podría conducir a estimaciones sesgadas o erróneas.

## 1.1 Interpretación general del modelo ajustado

Cuando se incluyen varias covariables en un modelo de regresión, se obtienen medidas de efecto ajustadas, como el Odds Ratio ajustado (OR ajustado) en la regresión logística, o el riesgo relativo ajustado (RR ajustado) en la regresión de Cox. Estas medidas estiman la asociación entre una variable específica y el desenlace de interés, mientras se controla el efecto de las demás covariables incluidas en el modelo.

Por ejemplo, el OR ajustado para fallecer tras un ACV isquémico indica la fuerza de esta asociación independientemente de otros factores como la edad, el sexo o las comorbilidades del paciente.

En esta sesión aplicaremos tanto modelos univariados (no ajustados) como multivariados (ajustados), utilizando el dataset previamente analizados en sesión de regresión logística.

## 1.2 Selección de variables para el modelo multivariado (ajustado)

La selección de variables consiste en decidir cuáles variables incluir en un modelo a partir de una lista completa de predictores disponibles, eliminando aquellas que son irrelevantes o redundantes. El objetivo es construir un modelo que explique adecuadamente el desenlace y permita realizar predicciones precisas sin sobreajustar los datos.

Existen al menos dos enfoques principales para la selección de variables:

### **1.2.1 Selección automática**

Este método emplea algoritmos automáticos —disponibles en R— para determinar qué variables incluir en el modelo. Las técnicas automáticas de selección se basan en criterios estadísticos como los valores p o los coeficientes de regresión. Los algoritmos difieren principalmente en la estrategia que utilizan para evaluar la inclusión o exclusión de variables en el modelo final.

Dependiendo de la dirección del algoritmo (forward, backward o stepwise), el resultado será un subconjunto seleccionado de variables. Para comparar entre distintos modelos generados por estos algoritmos, puede utilizarse el Criterio de Información de Akaike (Akaike Information Criterion, AIC), que estima el error de predicción y, por tanto, la calidad relativa de los modelos estadísticos para un conjunto de datos dado. En términos simples, cuanto menor sea el valor del AIC, mejor es el modelo en términos de equilibrio entre ajuste y complejidad.

Hay al menos tres algoritmos de selección automática de variables:

1.  Eliminación hacia atrás (*Backward elimination*),

2.  Selección hacia adelante (*Forward selection*) y

3.  Selección paso a paso (*Stepwise selection*).

Cada uno de estos métodos tiene ventajas y limitaciones. Entre ellos, la selección paso a paso es una técnica ampliamente utilizada en investigaciones en ciencias de la salud, ya que combina procedimientos de selección hacia adelante y hacia atrás. Esto permite añadir o eliminar variables de manera iterativa en función de criterios estadísticos, optimizando el modelo en ambos sentidos.

Sin embargo, la selección automática de variables no debería realizarse de manera aislada; es recomendable complementarla con una evaluación de la multicolinealidad. La multicolinealidad ocurre cuando dos o más variables independientes están altamente correlacionadas, lo que puede distorsionar las estimaciones del modelo. Por ejemplo, no es apropiado incluir simultáneamente el recuento total de leucocitos y el recuento de neutrófilos, dado que ambas variables están estrechamente relacionadas; en estos casos, es preferible seleccionar solo una de ellas.

En regresión, una herramienta común para detectar multicolinealidad es el Factor de Inflación de la Varianza (VIF, por sus siglas en inglés). De manera general, se interpreta así:

-   VIF de 1 indica que no hay multicolinealidad.
-   VIF entre 1 y 5 sugiere una multicolinealidad moderada.
-   VIF superior a 5 o 10 indica una multicolinealidad alta que puede requerir atención.

### **1.2.2 Selección intencionada de variables**

La selección intencionada de variables sigue una serie de pasos que combinan criterios estadísticos y consideraciones clínicas. Estos pasos incluyen:

-   Evaluación univariada de variables: Se realiza un análisis univariado para cada variable independiente con respecto a la variable de desenlace. Las variables que presentan una asociación estadísticamente significativa (habitualmente con un valor de p menor a 0.20) o que son consideradas clínicamente relevantes se seleccionan para su inclusión inicial en el modelo multivariado, independientemente de su significancia estadística.

-   Comparación de modelos multivariados: Las variables seleccionadas se incluyen en un modelo multivariado preliminar. A partir de este modelo, las variables que no alcanzan un nivel de significancia estadística estricto (por ejemplo, p \> 0.05) pueden ser consideradas para eliminación. Posteriormente, se comparan el modelo original (con todas las variables) y el modelo reducido (con las variables eliminadas) para evaluar si la simplificación del modelo afecta negativamente su capacidad explicativa o predictiva. Esta comparación puede realizarse mediante pruebas como la de razón de verosimilitud (Likelihood Ratio Test) o criterios de información (AIC/BIC).

-   Evaluación de interacciones: Es importante explorar posibles términos de interacción entre variables que, en combinación, podrían modificar el efecto sobre el desenlace.

## 2. Ejemplos de análisis univariado y multivariado en una regresión logística

### 2.1 El dataset para este ejercicio

Para ilustrar el proceso de análisis multivariado en un modelo de regresión logística, se empleará el dataset `almac_sangre`. Este conjunto de datos incluye información de 316 pacientes sometidos a cirugía por cáncer de próstata. Las variables registradas comprenden características sociodemográficas (como la edad y raza afroamericana), antecedentes médicos (historia familiar de cáncer), y factores clínicos y anatomopatológicos relevantes, tales como el volumen prostático, estadificación tumoral (Estadio T), puntajes de Gleason prequirúrgicos y quirúrgicos, PSA preoperatorio, presencia de confinamiento al órgano, necesidad de terapia adyuvante o radioterapia adyuvante, así como indicadores de recurrencia bioquímica y datos sobre el tiempo hasta la recurrencia. También se incluyen variables como el número de unidades transfundidas y si los ganglios linfáticos fueron positivos.

Cargando los datos

```{r}
sangre_data <- import(here("data", "almac_sangre.csv"))
```

Un vistazo a los datos

```{r}
head(sangre_data)
```

### 2.2 El análisis univariado

En esta sección se estimarán los Odds Ratios (OR) de cada variable de manera independiente, es decir, sin ajuste por otras covariables.

Antes de realizar este análisis, es necesario definir las categorías de referencia para las variables categóricas mediante la función mutate() en combinación con relevel(). Este paso asegura que la interpretación de los OR se haga en relación con la categoría de referencia seleccionada. El resultado se guarda en un nuevo objeto llamado sangre_data

```{r}
sangre_data_1 <- sangre_data |>
  mutate(
    Recurrencia_bioquimica = relevel(as.factor(Recurrencia_bioquimica), ref = "No"),
    Gleason_quirurgico = as.factor(Gleason_quirurgico),
    Estadio_T = as.factor(Estadio_T),
    Confinamiento_organo = relevel(as.factor(Confinamiento_organo), ref = "Sí"),
    BN_positivo = relevel(as.factor(BN_positivo), ref = "No"),
    Historia_familiar = relevel(as.factor(Historia_familiar), ref = "No"),
    Terapia_adyuvante = relevel(as.factor(Terapia_adyuvante), ref = "No")
  ) |>
  na.omit()
```

Para obtener la tabla con los resultados del análisis univariado, se utiliza la función tbl_uvregression(), que permite generar tablas con las estimaciones de regresión logística para cada variable incluida. Entre sus argumentos se especifican el método de regresión, las variables a analizar, la familia de distribución (binomial para modelos logísticos), y opciones de presentación de los resultados como los intervalos de confianza, valores p y formato de los estimadores

```{r}
tabla_reg_log_univ <- sangre_data_1 |>
  tbl_uvregression(
    include = c(Edad, PSA_preoperatorio, Gleason_quirurgico,
                Estadio_T, BN_positivo),
    y = Recurrencia_bioquimica,
    method = glm,
    method.args = list(family = binomial),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      Edad ~ "Edad (años)",
      PSA_preoperatorio ~ "PSA preoperatorio (ng/mL)",
      Gleason_quirurgico ~ "Gleason quirúrgico",
      Estadio_T ~ "Estadio T",
      BN_positivo ~ "Ganglios linfáticos positivos"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**OR no ajustado**", p.value = "**valor p**")
```

```{r}
tabla_reg_log_univ
```

**¿Cómo interpretar?**

Para la interpretación de las variables categóricas (como Gleason quirúrgico, estadio tumoral o ganglios linfáticos positivos), se recomienda tener en cuenta que los odds ratios (OR) representan la razón de probabilidades de presentar recurrencia bioquímica en comparación con una categoría de referencia. En este caso, las categorías de referencia son: Gleason 0–6, estadio T1-T2a y ganglios negativos, respectivamente.

En cuanto a las variables numéricas, se observa que:

La variable PSA preoperatorio presenta un OR de 1.09 (IC 95%: 1.04–1.14), lo cual indica que por cada aumento de una unidad en el PSA, las probabilidades (odds) de presentar recurrencia bioquímica aumentan en un 9%. Esta asociación es estadísticamente significativa (valor p \< 0.001).

La variable edad no mostró una asociación significativa con la recurrencia (p = 0.687), a pesar de que su OR fue ligeramente superior a 1.

Respecto a las variables categóricas:

Los pacientes con Gleason 8–10 tuvieron un OR de 24.20 (IC 95%: 6.36–121.97), lo cual indica que las probabilidades de recurrencia son más de 24 veces mayores comparado con Gleason 0–6. Esta diferencia es altamente significativa (p \< 0.001).

Del mismo modo, aquellos con tumores en estadio T2b–T3 tuvieron un OR de 4.93 (IC 95%: 2.23–10.77), lo que sugiere una probabilidad casi 5 veces mayor de recurrencia bioquímica (p \< 0.001).

Finalmente, la presencia de ganglios linfáticos positivos se asoció significativamente con la recurrencia: los pacientes con ganglios positivos presentaron un OR de 3.05 (IC 95%: 1.00–8.48), es decir, el triple de riesgo de recurrencia en comparación con quienes no tenían ganglios afectados (p = 0.037).

### 2.3 El análisis multivariado

Para el análisis de regresión logística multivariada, se aplicó una estrategia de selección automática de variables utilizando tres enfoques: eliminación hacia atrás (*backward elimination*), selección hacia adelante (*forward selection*) y selección paso a paso (*stepwise selection)*.

**Paso 1. Ajuste del modelo inicial**

Ajustamos un modelo de regresión logística binaria que incluya todas las variables candidatas

```{r}
modelo_inicial <- glm(
  Recurrencia_bioquimica ~ PSA_preoperatorio + Gleason_quirurgico + 
    Estadio_T + BN_positivo,
  data = sangre_data_1,
  family = binomial(link = "logit")
)
```

**Paso 2a. Realizamos la selección de variables** usando la técnica Eliminación hacia atrás (Backward elimination).

```{r}
multi_backward <- modelo_inicial |>
  step(direction = "backward", trace = FALSE)
```

**Paso 2b. Realizamos la selección de variables** usando la técnica Selección hacia adelante (Forward selection).

```{r}
multi_forward <- modelo_inicial |>
  step(direction = "forward", trace = FALSE)
```

**Paso 3c. Realizamos la selección de variables** usando la técnica Selección paso a paso (Stepwise selection).

```{r}
multi_stepwise <- modelo_inicial |>
  step(direction = "both", trace = FALSE)
```

Los resultados de la selección de las variables para el modelo se han guardado en los objetos: multi_backward, multi_forward, y multi_stepwise. El siguiente paso es comparar los valores de AIC y la multicolinealidad entre las variables seleccionadas por cada uno de los modelos.

**Paso 3. Estimados el AIC para los modelos.**

Podemos visualizar el AIC y cuáles variables han sido seleccionadas en cada modelo, usando la función summary.

```{r}
summary(multi_backward)
```

PSA preoperatorio y Gleason 8–10 se confirman como predictores estadísticamente significativos de recurrencia.

El modelo tiene buen ajuste (AIC = 220.7) y una reducción considerable en la deviance (de 255.92 a 210.70).

La categoría "Gleason 7" mostró una tendencia al aumento del riesgo, pero no alcanzó significancia estadística.

```{r}
summary(multi_forward)
```

PSA preoperatorio, Gleason 8–10, y Gleason no asignado son los predictores más importantes.

El resto de variables (Gleason 7, estadio, ganglios) no alcanzaron significancia estadística.

El ajuste global es aceptable (AIC = 223.69), aunque ligeramente peor que el modelo multi_backward (AIC = 220.7).

```{r}
summary(multi_stepwise)
```

Este modelo representa el mejor equilibrio entre ajuste y simplicidad.

Las variables clave que predicen la recurrencia bioquímica son: PSA preoperatorio, Gleason 8–10 y Gleason no asignado.

Es el modelo que recomendaríamos como modelo final.

### **2.4 Conclusión**

Los modelos obtenidos mediante eliminación hacia atrás (*backward elimination*) y selección paso a paso (*stepwise selection*) presentaron el menor valor de AIC (220.7), lo que indica un mejor ajuste en comparación con el modelo generado mediante selección hacia adelante (*forward selection*), cuyo AIC fue ligeramente superior (223.69).

Además, ambos enfoques (backward y stepwise) seleccionaron el mismo conjunto de variables: PSA preoperatorio y Gleason quirúrgico, lo cual refuerza su relevancia como predictores de recurrencia bioquímica.

### 2.5 Evaluación de colinealidad

Finalmente, evaluamos la colinealidad usando la función `check_collinearity()` del paquete `performance`.

```{r}
performance::check_collinearity(multi_backward, ci = NULL)
```

Todos los VIF son cercanos a 1, lo que indica que no existe colinealidad preocupante entre las variables del modelo

El resultado confirma que PSA preoperatorio y Gleason quirúrgico aportan información independiente al modelo

```{r}
performance::check_collinearity(multi_forward, ci = NULL)
```

Todos los VIF son menores a 2, lo que indica que no hay colinealidad significativa entre las variables

La tolerancia también se encuentra dentro de límites aceptables (\> 0.1)

Esto sugiere que las variables del modelo aportan información independiente, y no hay redundancia preocupante

```{r}
performance::check_collinearity(multi_stepwise, ci = NULL)
```

Ambos predictores presentan VIF cercanos a 1, lo que indica una muy baja colinealidad.

Esto sugiere que PSA preoperatorio y Gleason quirúrgico aportan información completamente independiente, sin redundancia estadística.

### **2.6 Conclusión**

Los modelos generados mediante eliminación hacia atrás (*backward elimination*) y selección paso a paso (*stepwise selection*) mostraron valores de VIF bajos y cercanos a 1, lo que indica una baja colinealidad entre las variables incluidas

En cambio, el modelo obtenido con la técnica de selección hacia adelante (*forward selection*) presentó valores de VIF ligeramente mayores para algunas variables (por ejemplo, *Estadio_T* y *BN_positivo*, con VIF ≈ 1.27). Aunque estos valores no indican una colinealidad grave, sí reflejan un mayor solapamiento entre predictores que podría explicar por qué dichas variables no fueron seleccionadas en los modelos más parsimoniosos

Este hallazgo refuerza la robustez de los modelos obtenidos mediante backward y stepwise, los cuales descartaron automáticamente aquellas variables que no aportaban información estadísticamente relevante o independiente

### 2.7 Modelo final

Con base en los resultados de ajuste (AIC) y la evaluación de colinealidad (VIF), se concluye que el modelo óptimo es el obtenido mediante las técnicas de eliminación hacia atrás (*backward elimination*) o selección paso a paso (*stepwise selection*), ya que ambos produjeron exactamente el mismo conjunto de variables y el menor valor de AIC (220.7).

## 3 Reporte del análisis univariado y multivariado

Como en las sesiones anteriores, reportaremos los resultados del modelo final de regresión logística.

Tabla para los resultados de la regresión univariado no ajustado

```{r}
tabla_univ <- sangre_data_1 |>
  tbl_uvregression(
    include = c(Edad, PSA_preoperatorio, Gleason_quirurgico,
                Estadio_T, BN_positivo),
    y = Recurrencia_bioquimica,
    method = glm,
    method.args = list(family = binomial),
    exponentiate = TRUE,
    conf.int = TRUE,
    hide_n = TRUE,
    add_estimate_to_reference_rows = FALSE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      Edad ~ "Edad (años)",
      PSA_preoperatorio ~ "PSA preoperatorio (ng/mL)",
      Gleason_quirurgico ~ "Gleason quirúrgico",
      Estadio_T ~ "Estadio T",
      BN_positivo ~ "Ganglios linfáticos positivos"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**OR**", p.value = "**valor p**")
```

Tabla para los resultados de la regresión multivariable (ajustado)

```{r}
tabla_multi <- glm(
  Recurrencia_bioquimica ~ PSA_preoperatorio + Gleason_quirurgico,
  family = binomial(link = "logit"),
  data = sangre_data_1
) |>
  tbl_regression(
    exponentiate = TRUE,
    conf.int = TRUE,
    pvalue_fun = ~ style_pvalue(.x, digits = 3),
    estimate_fun = ~ style_number(.x, digits = 2),
    label = list(
      PSA_preoperatorio ~ "PSA preoperatorio (ng/mL)",
      Gleason_quirurgico ~ "Gleason quirúrgico"
    )
  ) |>
  bold_labels() |>
  bold_p(t = 0.05) |>
  modify_header(estimate = "**OR ajustado**", p.value = "**valor p**")
```

La tabla final la construimos usando la función `tbl_merge()`. De modo que la tabla del análisis univariado o no ajustado y multivariado o ajustado, se muestren lado a lado.

```{r}
tabla_final <- 
  tbl_merge(
    list(tabla_univ, tabla_multi),
    tab_spanner = c("**Univariado**", "**Multivariado**")
  )
```

```{r}
tabla_final
```

### **3.1 ¿Cómo interpretar?**

En el modelo de regresión logística ajustado, el PSA preoperatorio y el puntaje de Gleason quirúrgico se asociaron de manera significativa con la recurrencia bioquímica

Por cada unidad adicional de PSA preoperatorio, las probabilidade de recurrencia aumentan en un 6% (OR = 1.06; IC95%: 1.01–1.11; p = 0.020), lo cual indica una asociación positiva y estadísticamente significativa

Asimismo, los pacientes con Gleason 8–10 presentaron un riesgo aproximadamente 20 veces mayor de recurrencia en comparación con aquellos con Gleason 0–6 (OR = 20.53; IC95%: 5.27–104.76; p \< 0.001). De forma similar, los casos no asignados en Gleason mostraron un riesgo 11.6 veces mayor (p \< 0.001).

Por otro lado, aunque la categoría Gleason 7 presentó un OR mayor a 1, esta no alcanzó significancia estadística tras el ajuste (p = 0.123). Variables como edad, estadio tumoral y ganglios linfáticos positivos sí mostraron asociaciones significativas en el análisis univariado, pero no se mantuvieron en el modelo final ajustado, lo cual sugiere que su efecto está mediado o confinado a otras variables más fuertes
